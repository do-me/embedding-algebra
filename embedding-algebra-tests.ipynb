{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers scikit-learn numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mixedbread-ai/mxbai-embed-large-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot Product:\n",
      "Dot product between 'king' and analogy vector: 206.02333068847656\n",
      "Dot product between 'woman' and analogy vector: 179.44287109375\n",
      "Dot product between 'princess' and analogy vector: 177.8961181640625\n",
      "Dot product between 'queen' and analogy vector: 177.6486053466797\n",
      "Dot product between 'castle' and analogy vector: 116.86325073242188\n",
      "Dot product between 'prince' and analogy vector: 113.52368927001953\n",
      "Dot product between 'horse' and analogy vector: 113.3372802734375\n",
      "Dot product between 'person' and analogy vector: 110.6568374633789\n",
      "Dot product between 'apple' and analogy vector: 107.81037139892578\n",
      "Dot product between 'banana' and analogy vector: 103.31510925292969\n",
      "Dot product between 'basketball' and analogy vector: 101.27586364746094\n",
      "Dot product between 'clown' and analogy vector: 97.28660583496094\n",
      "Dot product between 'football' and analogy vector: 96.44972229003906\n",
      "Dot product between 'man' and analogy vector: 47.41835021972656\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Cosine Similarity:\n",
      "Cosine similarity between 'king' and analogy vector: 0.7420865297317505\n",
      "Cosine similarity between 'woman' and analogy vector: 0.6679535508155823\n",
      "Cosine similarity between 'queen' and analogy vector: 0.6367943286895752\n",
      "Cosine similarity between 'princess' and analogy vector: 0.6064033508300781\n",
      "Cosine similarity between 'person' and analogy vector: 0.4240642786026001\n",
      "Cosine similarity between 'castle' and analogy vector: 0.41255974769592285\n",
      "Cosine similarity between 'horse' and analogy vector: 0.39906030893325806\n",
      "Cosine similarity between 'prince' and analogy vector: 0.3888675272464752\n",
      "Cosine similarity between 'apple' and analogy vector: 0.3804171681404114\n",
      "Cosine similarity between 'banana' and analogy vector: 0.3553932309150696\n",
      "Cosine similarity between 'basketball' and analogy vector: 0.3550359904766083\n",
      "Cosine similarity between 'football' and analogy vector: 0.3462764620780945\n",
      "Cosine similarity between 'clown' and analogy vector: 0.3232797086238861\n",
      "Cosine similarity between 'man' and analogy vector: 0.17841237783432007\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Euclidean Distance (sorted by smallest distance, which indicates highest similarity):\n",
      "Euclidean distance between 'king' and analogy vector: 12.40994930267334\n",
      "Euclidean distance between 'woman' and analogy vector: 13.87999439239502\n",
      "Euclidean distance between 'queen' and analogy vector: 14.593585968017578\n",
      "Euclidean distance between 'princess' and analogy vector: 15.389604568481445\n",
      "Euclidean distance between 'person' and analogy vector: 17.837038040161133\n",
      "Euclidean distance between 'castle' and analogy vector: 18.484573364257812\n",
      "Euclidean distance between 'horse' and analogy vector: 18.707866668701172\n",
      "Euclidean distance between 'apple' and analogy vector: 18.974042892456055\n",
      "Euclidean distance between 'prince' and analogy vector: 19.055479049682617\n",
      "Euclidean distance between 'football' and analogy vector: 19.355772018432617\n",
      "Euclidean distance between 'basketball' and analogy vector: 19.39596176147461\n",
      "Euclidean distance between 'banana' and analogy vector: 19.529788970947266\n",
      "Euclidean distance between 'clown' and analogy vector: 20.282350540161133\n",
      "Euclidean distance between 'man' and analogy vector: 21.264333724975586\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\n",
    "\n",
    "instruction = \"Represent this sentence for searching relevant passages: \"\n",
    "\n",
    "# Words to test the analogy\n",
    "words = [\"king\", \"man\", \"woman\", \"queen\", \"princess\", \"prince\", \"clown\", \"horse\", \"castle\", \"person\", \"banana\", \"apple\", \"football\", \"basketball\"]\n",
    "\n",
    "# Encode the words to get their embeddings and store them in a dictionary\n",
    "embeddings_dict = {word: model.encode(instruction + word) for word in words}\n",
    "\n",
    "# Perform the analogy calculation: king - man + woman\n",
    "analogy_vector = embeddings_dict[\"king\"] - embeddings_dict[\"man\"] + embeddings_dict[\"woman\"]\n",
    "\n",
    "# Calculate similarity scores and store them in lists\n",
    "dot_product_scores = []\n",
    "cosine_similarity_scores = []\n",
    "euclidean_distance_scores = []\n",
    "\n",
    "for word, embedding in embeddings_dict.items():\n",
    "    # Dot Product\n",
    "    dot_product = np.dot(analogy_vector, embedding)\n",
    "    dot_product_scores.append((word, dot_product))\n",
    "\n",
    "    # Cosine Similarity\n",
    "    cosine_sim = cosine_similarity(analogy_vector.reshape(1, -1), embedding.reshape(1, -1))[0][0]\n",
    "    cosine_similarity_scores.append((word, cosine_sim))\n",
    "\n",
    "    # Euclidean Distance (use negative for sorting purposes, since lower distance means higher similarity)\n",
    "    euclidean_distance = -np.linalg.norm(analogy_vector - embedding)\n",
    "    euclidean_distance_scores.append((word, euclidean_distance))\n",
    "\n",
    "# Sort the lists by scores\n",
    "sorted_dot_product_scores = sorted(dot_product_scores, key=lambda x: x[1], reverse=True)\n",
    "sorted_cosine_similarity_scores = sorted(cosine_similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "sorted_euclidean_distance_scores = sorted(euclidean_distance_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted results for Dot Product\n",
    "print(\"Dot Product:\")\n",
    "for word, score in sorted_dot_product_scores:\n",
    "    print(f\"Dot product between '{word}' and analogy vector: {score}\")\n",
    "\n",
    "# Print a separator\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# Print the sorted results for Cosine Similarity\n",
    "print(\"Cosine Similarity:\")\n",
    "for word, score in sorted_cosine_similarity_scores:\n",
    "    print(f\"Cosine similarity between '{word}' and analogy vector: {score}\")\n",
    "\n",
    "# Print a separator\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# Print the sorted results for Euclidean Distance\n",
    "print(\"Euclidean Distance (sorted by smallest distance, which indicates highest similarity):\")\n",
    "for word, score in sorted_euclidean_distance_scores:\n",
    "    print(f\"Euclidean distance between '{word}' and analogy vector: {-score}\")  # Multiply by -1 to show the original positive distanceb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAAI/bge-base-en-v1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot Product:\n",
      "Dot product between 'king' and analogy vector: 0.8582403659820557\n",
      "Dot product between 'woman' and analogy vector: 0.7982355356216431\n",
      "Dot product between 'queen' and analogy vector: 0.7474974393844604\n",
      "Dot product between 'princess' and analogy vector: 0.6489111185073853\n",
      "Dot product between 'castle' and analogy vector: 0.4740004539489746\n",
      "Dot product between 'person' and analogy vector: 0.4641091525554657\n",
      "Dot product between 'banana' and analogy vector: 0.4288800358772278\n",
      "Dot product between 'basketball' and analogy vector: 0.42559313774108887\n",
      "Dot product between 'apple' and analogy vector: 0.4131776690483093\n",
      "Dot product between 'clown' and analogy vector: 0.3922470510005951\n",
      "Dot product between 'horse' and analogy vector: 0.3739059567451477\n",
      "Dot product between 'prince' and analogy vector: 0.34641796350479126\n",
      "Dot product between 'football' and analogy vector: 0.296387255191803\n",
      "Dot product between 'man' and analogy vector: 0.16995564103126526\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Cosine Similarity:\n",
      "Cosine similarity between 'king' and analogy vector: 0.7039203643798828\n",
      "Cosine similarity between 'woman' and analogy vector: 0.6547049283981323\n",
      "Cosine similarity between 'queen' and analogy vector: 0.6130900382995605\n",
      "Cosine similarity between 'princess' and analogy vector: 0.5322304964065552\n",
      "Cosine similarity between 'castle' and analogy vector: 0.38877052068710327\n",
      "Cosine similarity between 'person' and analogy vector: 0.38065776228904724\n",
      "Cosine similarity between 'banana' and analogy vector: 0.35176318883895874\n",
      "Cosine similarity between 'basketball' and analogy vector: 0.3490673303604126\n",
      "Cosine similarity between 'apple' and analogy vector: 0.33888423442840576\n",
      "Cosine similarity between 'clown' and analogy vector: 0.32171714305877686\n",
      "Cosine similarity between 'horse' and analogy vector: 0.3066740036010742\n",
      "Cosine similarity between 'prince' and analogy vector: 0.2841286361217499\n",
      "Cosine similarity between 'football' and analogy vector: 0.24309392273426056\n",
      "Cosine similarity between 'man' and analogy vector: 0.13939596712589264\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Euclidean Distance (sorted by smallest distance, which indicates highest similarity):\n",
      "Euclidean distance between 'king' and analogy vector: 0.877518892288208\n",
      "Euclidean distance between 'woman' and analogy vector: 0.9434242248535156\n",
      "Euclidean distance between 'queen' and analogy vector: 0.9957536458969116\n",
      "Euclidean distance between 'princess' and analogy vector: 1.0902742147445679\n",
      "Euclidean distance between 'castle' and analogy vector: 1.240370512008667\n",
      "Euclidean distance between 'person' and analogy vector: 1.2483196258544922\n",
      "Euclidean distance between 'banana' and analogy vector: 1.276228904724121\n",
      "Euclidean distance between 'basketball' and analogy vector: 1.2788017988204956\n",
      "Euclidean distance between 'apple' and analogy vector: 1.2884738445281982\n",
      "Euclidean distance between 'clown' and analogy vector: 1.3046172857284546\n",
      "Euclidean distance between 'horse' and analogy vector: 1.3186008930206299\n",
      "Euclidean distance between 'prince' and analogy vector: 1.3392850160598755\n",
      "Euclidean distance between 'football' and analogy vector: 1.3761342763900757\n",
      "Euclidean distance between 'man' and analogy vector: 1.4651310443878174\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "instruction = \"Represent this sentence for searching relevant passages: \"\n",
    "\n",
    "# Words to test the analogy\n",
    "words = [\"king\", \"man\", \"woman\", \"queen\", \"princess\", \"prince\", \"clown\", \"horse\", \"castle\", \"person\", \"banana\", \"apple\", \"football\", \"basketball\"]\n",
    "\n",
    "# Encode the words to get their embeddings and store them in a dictionary\n",
    "embeddings_dict = {word: model.encode(instruction + word) for word in words}\n",
    "\n",
    "# Perform the analogy calculation: king - man + woman\n",
    "analogy_vector = embeddings_dict[\"king\"] - embeddings_dict[\"man\"] + embeddings_dict[\"woman\"]\n",
    "\n",
    "# Calculate similarity scores and store them in lists\n",
    "dot_product_scores = []\n",
    "cosine_similarity_scores = []\n",
    "euclidean_distance_scores = []\n",
    "\n",
    "for word, embedding in embeddings_dict.items():\n",
    "    # Dot Product\n",
    "    dot_product = np.dot(analogy_vector, embedding)\n",
    "    dot_product_scores.append((word, dot_product))\n",
    "\n",
    "    # Cosine Similarity\n",
    "    cosine_sim = cosine_similarity(analogy_vector.reshape(1, -1), embedding.reshape(1, -1))[0][0]\n",
    "    cosine_similarity_scores.append((word, cosine_sim))\n",
    "\n",
    "    # Euclidean Distance (use negative for sorting purposes, since lower distance means higher similarity)\n",
    "    euclidean_distance = -np.linalg.norm(analogy_vector - embedding)\n",
    "    euclidean_distance_scores.append((word, euclidean_distance))\n",
    "\n",
    "# Sort the lists by scores\n",
    "sorted_dot_product_scores = sorted(dot_product_scores, key=lambda x: x[1], reverse=True)\n",
    "sorted_cosine_similarity_scores = sorted(cosine_similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "sorted_euclidean_distance_scores = sorted(euclidean_distance_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted results for Dot Product\n",
    "print(\"Dot Product:\")\n",
    "for word, score in sorted_dot_product_scores:\n",
    "    print(f\"Dot product between '{word}' and analogy vector: {score}\")\n",
    "\n",
    "# Print a separator\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# Print the sorted results for Cosine Similarity\n",
    "print(\"Cosine Similarity:\")\n",
    "for word, score in sorted_cosine_similarity_scores:\n",
    "    print(f\"Cosine similarity between '{word}' and analogy vector: {score}\")\n",
    "\n",
    "# Print a separator\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# Print the sorted results for Euclidean Distance\n",
    "print(\"Euclidean Distance (sorted by smallest distance, which indicates highest similarity):\")\n",
    "for word, score in sorted_euclidean_distance_scores:\n",
    "    print(f\"Euclidean distance between '{word}' and analogy vector: {-score}\")  # Multiply by -1 to show the original positive distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all-MiniLM-L6-v2 (without instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot Product:\n",
      "Dot product between 'king' and analogy vector: 0.9423492550849915\n",
      "Dot product between 'woman' and analogy vector: 0.9383166432380676\n",
      "Dot product between 'queen' and analogy vector: 0.8659945130348206\n",
      "Dot product between 'princess' and analogy vector: 0.660201370716095\n",
      "Dot product between 'prince' and analogy vector: 0.5800817012786865\n",
      "Dot product between 'person' and analogy vector: 0.5365455150604248\n",
      "Dot product between 'castle' and analogy vector: 0.5108327865600586\n",
      "Dot product between 'clown' and analogy vector: 0.4597344994544983\n",
      "Dot product between 'banana' and analogy vector: 0.45373284816741943\n",
      "Dot product between 'horse' and analogy vector: 0.3839447498321533\n",
      "Dot product between 'basketball' and analogy vector: 0.33809804916381836\n",
      "Dot product between 'apple' and analogy vector: 0.3302672207355499\n",
      "Dot product between 'football' and analogy vector: 0.2978869080543518\n",
      "Dot product between 'man' and analogy vector: -0.3526756763458252\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Cosine Similarity:\n",
      "Cosine similarity between 'king' and analogy vector: 0.6305714845657349\n",
      "Cosine similarity between 'woman' and analogy vector: 0.6278731822967529\n",
      "Cosine similarity between 'queen' and analogy vector: 0.5794788599014282\n",
      "Cosine similarity between 'princess' and analogy vector: 0.4417726993560791\n",
      "Cosine similarity between 'prince' and analogy vector: 0.38816073536872864\n",
      "Cosine similarity between 'person' and analogy vector: 0.35902857780456543\n",
      "Cosine similarity between 'castle' and analogy vector: 0.3418229818344116\n",
      "Cosine similarity between 'clown' and analogy vector: 0.3076305687427521\n",
      "Cosine similarity between 'banana' and analogy vector: 0.3036145865917206\n",
      "Cosine similarity between 'horse' and analogy vector: 0.25691601634025574\n",
      "Cosine similarity between 'basketball' and analogy vector: 0.22623780369758606\n",
      "Cosine similarity between 'apple' and analogy vector: 0.22099778056144714\n",
      "Cosine similarity between 'football' and analogy vector: 0.19933053851127625\n",
      "Cosine similarity between 'man' and analogy vector: -0.2359924018383026\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Euclidean Distance (sorted by smallest distance, which indicates highest similarity):\n",
      "Euclidean distance between 'king' and analogy vector: 1.1613109111785889\n",
      "Euclidean distance between 'woman' and analogy vector: 1.164778232574463\n",
      "Euclidean distance between 'queen' and analogy vector: 1.2252968549728394\n",
      "Euclidean distance between 'princess' and analogy vector: 1.3830903768539429\n",
      "Euclidean distance between 'prince' and analogy vector: 1.439853549003601\n",
      "Euclidean distance between 'person' and analogy vector: 1.469779133796692\n",
      "Euclidean distance between 'castle' and analogy vector: 1.4871704578399658\n",
      "Euclidean distance between 'clown' and analogy vector: 1.521141767501831\n",
      "Euclidean distance between 'banana' and analogy vector: 1.5250822305679321\n",
      "Euclidean distance between 'horse' and analogy vector: 1.5701758861541748\n",
      "Euclidean distance between 'basketball' and analogy vector: 1.5991076231002808\n",
      "Euclidean distance between 'apple' and analogy vector: 1.6039973497390747\n",
      "Euclidean distance between 'football' and analogy vector: 1.6240590810775757\n",
      "Euclidean distance between 'man' and analogy vector: 1.9846140146255493\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "instruction = \"\" #\"Represent this sentence for searching relevant passages: \"\n",
    "\n",
    "# Words to test the analogy\n",
    "words = [\"king\", \"man\", \"woman\", \"queen\", \"princess\", \"prince\", \"clown\", \"horse\", \"castle\", \"person\", \"banana\", \"apple\", \"football\", \"basketball\"]\n",
    "\n",
    "# Encode the words to get their embeddings and store them in a dictionary\n",
    "embeddings_dict = {word: model.encode(instruction + word) for word in words}\n",
    "\n",
    "# Perform the analogy calculation: king - man + woman\n",
    "analogy_vector = embeddings_dict[\"king\"] - embeddings_dict[\"man\"] + embeddings_dict[\"woman\"]\n",
    "\n",
    "# Calculate similarity scores and store them in lists\n",
    "dot_product_scores = []\n",
    "cosine_similarity_scores = []\n",
    "euclidean_distance_scores = []\n",
    "\n",
    "for word, embedding in embeddings_dict.items():\n",
    "    # Dot Product\n",
    "    dot_product = np.dot(analogy_vector, embedding)\n",
    "    dot_product_scores.append((word, dot_product))\n",
    "\n",
    "    # Cosine Similarity\n",
    "    cosine_sim = cosine_similarity(analogy_vector.reshape(1, -1), embedding.reshape(1, -1))[0][0]\n",
    "    cosine_similarity_scores.append((word, cosine_sim))\n",
    "\n",
    "    # Euclidean Distance (use negative for sorting purposes, since lower distance means higher similarity)\n",
    "    euclidean_distance = -np.linalg.norm(analogy_vector - embedding)\n",
    "    euclidean_distance_scores.append((word, euclidean_distance))\n",
    "\n",
    "# Sort the lists by scores\n",
    "sorted_dot_product_scores = sorted(dot_product_scores, key=lambda x: x[1], reverse=True)\n",
    "sorted_cosine_similarity_scores = sorted(cosine_similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "sorted_euclidean_distance_scores = sorted(euclidean_distance_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted results for Dot Product\n",
    "print(\"Dot Product:\")\n",
    "for word, score in sorted_dot_product_scores:\n",
    "    print(f\"Dot product between '{word}' and analogy vector: {score}\")\n",
    "\n",
    "# Print a separator\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# Print the sorted results for Cosine Similarity\n",
    "print(\"Cosine Similarity:\")\n",
    "for word, score in sorted_cosine_similarity_scores:\n",
    "    print(f\"Cosine similarity between '{word}' and analogy vector: {score}\")\n",
    "\n",
    "# Print a separator\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# Print the sorted results for Euclidean Distance\n",
    "print(\"Euclidean Distance (sorted by smallest distance, which indicates highest similarity):\")\n",
    "for word, score in sorted_euclidean_distance_scores:\n",
    "    print(f\"Euclidean distance between '{word}' and analogy vector: {-score}\")  # Multiply by -1 to show the original positive distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all-MiniLM-L6-v2 (with instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot Product:\n",
      "Dot product between 'king' and analogy vector: 0.9228507876396179\n",
      "Dot product between 'queen' and analogy vector: 0.8901626467704773\n",
      "Dot product between 'woman' and analogy vector: 0.8280522227287292\n",
      "Dot product between 'princess' and analogy vector: 0.8019042015075684\n",
      "Dot product between 'prince' and analogy vector: 0.6872838139533997\n",
      "Dot product between 'castle' and analogy vector: 0.6797710657119751\n",
      "Dot product between 'person' and analogy vector: 0.5742331147193909\n",
      "Dot product between 'horse' and analogy vector: 0.5541369915008545\n",
      "Dot product between 'banana' and analogy vector: 0.5159429311752319\n",
      "Dot product between 'clown' and analogy vector: 0.510412871837616\n",
      "Dot product between 'basketball' and analogy vector: 0.5029580593109131\n",
      "Dot product between 'apple' and analogy vector: 0.47977399826049805\n",
      "Dot product between 'football' and analogy vector: 0.47085604071617126\n",
      "Dot product between 'man' and analogy vector: 0.4366154074668884\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Cosine Similarity:\n",
      "Cosine similarity between 'king' and analogy vector: 0.8049821853637695\n",
      "Cosine similarity between 'queen' and analogy vector: 0.7764691114425659\n",
      "Cosine similarity between 'woman' and analogy vector: 0.7222915291786194\n",
      "Cosine similarity between 'princess' and analogy vector: 0.6994831562042236\n",
      "Cosine similarity between 'prince' and analogy vector: 0.599502444267273\n",
      "Cosine similarity between 'castle' and analogy vector: 0.5929491519927979\n",
      "Cosine similarity between 'person' and analogy vector: 0.5008907318115234\n",
      "Cosine similarity between 'horse' and analogy vector: 0.4833613336086273\n",
      "Cosine similarity between 'banana' and analogy vector: 0.45004552602767944\n",
      "Cosine similarity between 'clown' and analogy vector: 0.44522178173065186\n",
      "Cosine similarity between 'basketball' and analogy vector: 0.43871909379959106\n",
      "Cosine similarity between 'apple' and analogy vector: 0.41849619150161743\n",
      "Cosine similarity between 'football' and analogy vector: 0.4107171893119812\n",
      "Cosine similarity between 'man' and analogy vector: 0.3808498978614807\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Euclidean Distance (sorted by smallest distance, which indicates highest similarity):\n",
      "Euclidean distance between 'king' and analogy vector: 0.6845334768295288\n",
      "Euclidean distance between 'queen' and analogy vector: 0.7307273149490356\n",
      "Euclidean distance between 'woman' and analogy vector: 0.8112848401069641\n",
      "Euclidean distance between 'princess' and analogy vector: 0.8428993821144104\n",
      "Euclidean distance between 'prince' and analogy vector: 0.9693915247917175\n",
      "Euclidean distance between 'castle' and analogy vector: 0.9771108627319336\n",
      "Euclidean distance between 'person' and analogy vector: 1.0797321796417236\n",
      "Euclidean distance between 'horse' and analogy vector: 1.0981864929199219\n",
      "Euclidean distance between 'banana' and analogy vector: 1.1324318647384644\n",
      "Euclidean distance between 'clown' and analogy vector: 1.137304663658142\n",
      "Euclidean distance between 'basketball' and analogy vector: 1.1438406705856323\n",
      "Euclidean distance between 'apple' and analogy vector: 1.1639328002929688\n",
      "Euclidean distance between 'football' and analogy vector: 1.1715697050094604\n",
      "Euclidean distance between 'man' and analogy vector: 1.2004401683807373\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "instruction = \"Represent this sentence for searching relevant passages: \"\n",
    "\n",
    "# Words to test the analogy\n",
    "words = [\"king\", \"man\", \"woman\", \"queen\", \"princess\", \"prince\", \"clown\", \"horse\", \"castle\", \"person\", \"banana\", \"apple\", \"football\", \"basketball\"]\n",
    "\n",
    "# Encode the words to get their embeddings and store them in a dictionary\n",
    "embeddings_dict = {word: model.encode(instruction + word) for word in words}\n",
    "\n",
    "# Perform the analogy calculation: king - man + woman\n",
    "analogy_vector = embeddings_dict[\"king\"] - embeddings_dict[\"man\"] + embeddings_dict[\"woman\"]\n",
    "\n",
    "# Calculate similarity scores and store them in lists\n",
    "dot_product_scores = []\n",
    "cosine_similarity_scores = []\n",
    "euclidean_distance_scores = []\n",
    "\n",
    "for word, embedding in embeddings_dict.items():\n",
    "    # Dot Product\n",
    "    dot_product = np.dot(analogy_vector, embedding)\n",
    "    dot_product_scores.append((word, dot_product))\n",
    "\n",
    "    # Cosine Similarity\n",
    "    cosine_sim = cosine_similarity(analogy_vector.reshape(1, -1), embedding.reshape(1, -1))[0][0]\n",
    "    cosine_similarity_scores.append((word, cosine_sim))\n",
    "\n",
    "    # Euclidean Distance (use negative for sorting purposes, since lower distance means higher similarity)\n",
    "    euclidean_distance = -np.linalg.norm(analogy_vector - embedding)\n",
    "    euclidean_distance_scores.append((word, euclidean_distance))\n",
    "\n",
    "# Sort the lists by scores\n",
    "sorted_dot_product_scores = sorted(dot_product_scores, key=lambda x: x[1], reverse=True)\n",
    "sorted_cosine_similarity_scores = sorted(cosine_similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "sorted_euclidean_distance_scores = sorted(euclidean_distance_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted results for Dot Product\n",
    "print(\"Dot Product:\")\n",
    "for word, score in sorted_dot_product_scores:\n",
    "    print(f\"Dot product between '{word}' and analogy vector: {score}\")\n",
    "\n",
    "# Print a separator\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# Print the sorted results for Cosine Similarity\n",
    "print(\"Cosine Similarity:\")\n",
    "for word, score in sorted_cosine_similarity_scores:\n",
    "    print(f\"Cosine similarity between '{word}' and analogy vector: {score}\")\n",
    "\n",
    "# Print a separator\n",
    "print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# Print the sorted results for Euclidean Distance\n",
    "print(\"Euclidean Distance (sorted by smallest distance, which indicates highest similarity):\")\n",
    "for word, score in sorted_euclidean_distance_scores:\n",
    "    print(f\"Euclidean distance between '{word}' and analogy vector: {-score}\")  # Multiply by -1 to show the original positive distance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
